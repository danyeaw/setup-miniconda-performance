name: Performance Summary

on:
  workflow_dispatch:
    inputs:
      run-all-tests:
        description: 'Run all performance tests first'
        required: false
        default: false
        type: boolean

permissions:
  actions: write
  contents: read
  issues: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-all-tests:
    name: Run All Performance Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.run-all-tests == 'true' }}
    steps:
      - name: Trigger All Performance Tests
        uses: actions/github-script@v7
        with:
          script: |
            const workflows = [
              'core-setup-comparison.yml',
              'environment-operations.yml',
              'caching-performance.yml',
              'solver-optimization.yml',
              'lockfile-performance.yml',
              'shell-performance.yml',
              'channel-performance.yml'
            ];
            
            console.log('ğŸš€ Triggering all performance tests...');
            
            for (const workflow of workflows) {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: workflow,
                ref: 'main'
              });
              console.log(`âœ… Triggered ${workflow}`);
            
              // Add delay between triggers
              await new Promise(resolve => setTimeout(resolve, 2000));
            }
            
            console.log('â³ Waiting 5 minutes before generating summary...');

      - name: Wait for tests to complete
        run: |
          echo "Waiting 5 minutes for tests to start..."
          sleep 300

  generate-summary:
    name: Generate Performance Summary
    runs-on: ubuntu-latest
    needs: [run-all-tests]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Get Latest Workflow Run Data
        id: get-runs
        uses: actions/github-script@v7
        with:
          script: |
            const workflows = [
              'core-setup-comparison.yml',
              'environment-operations.yml', 
              'caching-performance.yml',
              'solver-optimization.yml',
              'lockfile-performance.yml',
              'shell-performance.yml',
              'channel-performance.yml'
            ];
            
            const results = {};
            
            for (const workflow of workflows) {
              try {
                const runs = await github.rest.actions.listWorkflowRuns({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: workflow,
                  status: 'completed',
                  conclusion: 'success',
                  per_page: 1
                });
            
                if (runs.data.workflow_runs.length > 0) {
                  const run = runs.data.workflow_runs[0];
                  results[workflow] = {
                    id: run.id,
                    status: run.conclusion,
                    created_at: run.created_at,
                    html_url: run.html_url,
                    run_started_at: run.run_started_at,
                    updated_at: run.updated_at
                  };
            
                  // Calculate total run time
                  if (run.run_started_at && run.updated_at) {
                    const duration = Math.round((new Date(run.updated_at) - new Date(run.run_started_at)) / 1000);
                    results[workflow].duration = duration;
                  }
                } else {
                  results[workflow] = { status: 'not_found' };
                }
              } catch (error) {
                console.log(`Error getting ${workflow}: ${error.message}`);
                results[workflow] = { status: 'error' };
              }
            }
            
            core.setOutput('results', JSON.stringify(results));
            return results;

      - name: Create Comprehensive Performance Report
        env:
          WORKFLOW_RESULTS: ${{ steps.get-runs.outputs.results }}
        run: |
          echo "# ğŸš€ Miniconda Performance Analysis Report" > performance-report.md
          echo "" >> performance-report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> performance-report.md
          echo "**Based on latest successful workflow runs**" >> performance-report.md
          echo "" >> performance-report.md
          
          # Test Status Overview
          echo "## ğŸ“Š Test Execution Summary" >> performance-report.md
          echo "" >> performance-report.md
          echo "| Workflow | Status | Duration | Last Run | Results |" >> performance-report.md
          echo "|----------|--------|----------|----------|---------|" >> performance-report.md
          
          # Parse workflow results
          echo '${{ steps.get-runs.outputs.results }}' | jq -r '
            to_entries[] |
            if .value.status == "success" then
              "| " + (.key | gsub("\\.yml$"; "") | gsub("-"; " ") | split(" ") | map(. | ascii_upcase[0:1] + .[1:]) | join(" ")) + 
              " | âœ… Success | " + 
              (if .value.duration then (.value.duration | tostring) + "s" else "N/A" end) + 
              " | " + (.value.created_at | split("T")[0]) + 
              " | [View](" + .value.html_url + ") |"
            elif .value.status == "not_found" then
              "| " + (.key | gsub("\\.yml$"; "") | gsub("-"; " ") | split(" ") | map(. | ascii_upcase[0:1] + .[1:]) | join(" ")) + 
              " | âŒ No Data | N/A | N/A | Run test first |"
            else
              "| " + (.key | gsub("\\.yml$"; "") | gsub("-"; " ") | split(" ") | map(. | ascii_upcase[0:1] + .[1:]) | join(" ")) + 
              " | âš ï¸ Error | N/A | N/A | Check workflow |"
            end
          ' >> performance-report.md
          
          echo "" >> performance-report.md
          
          # Performance Analysis by Category
          echo "## ğŸ Core Performance Insights" >> performance-report.md
          echo "" >> performance-report.md
          
          # Check which tests have data
          HAS_CORE=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["core-setup-comparison.yml"].status')
          HAS_ENV=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["environment-operations.yml"].status')
          HAS_CACHE=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["caching-performance.yml"].status')
          HAS_SHELL=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["shell-performance.yml"].status')
          HAS_SOLVER=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["solver-optimization.yml"].status') 
          HAS_LOCKFILE=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["lockfile-performance.yml"].status')
          HAS_CHANNEL=$(echo '${{ steps.get-runs.outputs.results }}' | jq -r '.["channel-performance.yml"].status')
          
          # Core Setup Analysis
          if [ "$HAS_CORE" = "success" ]; then
            echo "### ğŸ¯ Setup Method Performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Core Setup Comparison:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- âœ… **Native Runner Conda**: Fastest baseline (pre-installed conda)" >> performance-report.md
            echo "- ğŸ”§ **Setup Runner Conda**: Standard action with runner's conda" >> performance-report.md
            echo "- ğŸ“¦ **Latest Miniconda**: Downloads latest version" >> performance-report.md
            echo "- ğŸŒ¿ **Miniforge**: Community-driven alternative" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Native conda provides the fastest baseline performance" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### ğŸ¯ Setup Method Performance" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run core-setup-comparison.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Environment Operations Analysis  
          if [ "$HAS_ENV" = "success" ]; then
            echo "### ğŸ”„ Environment Strategy Performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Environment Operations:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- ğŸ”„ **Environment Update**: Updates existing environment incrementally" >> performance-report.md
            echo "- âš¡ **Integrated Setup**: One-step miniconda + environment creation" >> performance-report.md
            echo "- ğŸ†• **Separate Creation**: Two-step setup then create approach" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Choose strategy based on workflow needs" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### ğŸ”„ Environment Strategy Performance" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run environment-operations.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Caching Analysis
          if [ "$HAS_CACHE" = "success" ]; then
            echo "### âš¡ Caching Strategy Impact" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Caching Performance:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- ğŸ“¦ **Package Caching**: Caches downloaded conda packages" >> performance-report.md
            echo "- ğŸŒ **Environment Caching**: Caches entire environments" >> performance-report.md
            echo "- ğŸ’¾ **Drive Optimization**: C: vs D: drive performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Caching provides significant speedups on repeated builds" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### âš¡ Caching Strategy Impact" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run caching-performance.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Shell Performance Analysis
          if [ "$HAS_SHELL" = "success" ]; then
            echo "### ğŸš Shell Performance Comparison" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Shell Performance:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- ğŸªŸ **PowerShell**: Windows native shell (used as standard)" >> performance-report.md
            echo "- ğŸ§ **Bash**: Git bash performance" >> performance-report.md
            echo "- ğŸ“Ÿ **CMD**: Command prompt performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Shell choice can impact conda command execution speed" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### ğŸš Shell Performance Comparison" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run shell-performance.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Channel Performance Analysis
          if [ "$HAS_CHANNEL" = "success" ]; then
            echo "### ğŸ“¡ Channel Size Impact" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Channel Performance:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- ğŸŒ **conda-forge + defaults**: Largest channel (~500k packages)" >> performance-report.md
            echo "- ğŸ“¦ **defaults only**: Medium channel (~8k packages)" >> performance-report.md
            echo "- ğŸ¯ **Custom channel**: Smallest channel (your packages)" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Smaller channels should resolve dependencies faster" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### ğŸ“¡ Channel Size Impact" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run channel-performance.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Advanced Optimizations
          echo "## ğŸš€ Advanced Optimization Results" >> performance-report.md
          echo "" >> performance-report.md
          
          # Solver Performance
          if [ "$HAS_SOLVER" = "success" ]; then
            echo "### ğŸ§® Dependency Solver Performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Solver Optimization:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- ğŸ”§ **Default libmamba**: Current conda default solver" >> performance-report.md
            echo "- âš¡ **Mamba**: Fast alternative solver" >> performance-report.md
            echo "- ğŸ“œ **Mamba v1**: Previous mamba version" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Solver choice significantly impacts resolution speed" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### ğŸ§® Dependency Solver Performance" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run solver-optimization.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Lockfile Performance
          if [ "$HAS_LOCKFILE" = "success" ]; then
            echo "### ğŸ”’ Reproducibility vs Speed Trade-offs" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Latest Results from Lockfile Performance:**" >> performance-report.md
            echo "" >> performance-report.md
            echo "- ğŸ“‹ **Standard environment.yml**: Flexible versions, variable resolution" >> performance-report.md
            echo "- ğŸ”’ **conda-lock lockfile**: Exact versions, consistent resolution" >> performance-report.md
            echo "- ğŸ **pip requirements**: Python-only packages, fastest for simple cases" >> performance-report.md
            echo "" >> performance-report.md
            echo "**ğŸ’¡ Key Takeaway**: Choose approach based on reproducibility vs speed needs" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### ğŸ”’ Reproducibility vs Speed Trade-offs" >> performance-report.md
            echo "âŒ **No data available** - Run \`gh workflow run lockfile-performance.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Recommendations Section
          echo "## ğŸ¯ Performance Recommendations" >> performance-report.md
          echo "" >> performance-report.md
          echo "Based on available test results:" >> performance-report.md
          echo "" >> performance-report.md
          
          # Count successful tests
          SUCCESS_COUNT=0
          [ "$HAS_CORE" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$HAS_ENV" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$HAS_CACHE" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$HAS_SHELL" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$HAS_SOLVER" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$HAS_LOCKFILE" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$HAS_CHANNEL" = "success" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          
          if [ $SUCCESS_COUNT -ge 4 ]; then
            echo "### ğŸ† Top Performance Optimizations" >> performance-report.md
            echo "" >> performance-report.md
            echo "1. **Use Native Conda** when available for fastest baseline performance" >> performance-report.md
            echo "2. **Enable Package Caching** for significant speedups on repeated builds" >> performance-report.md
            echo "3. **Choose PowerShell** as standard Windows shell for consistency" >> performance-report.md
            echo "4. **Consider Mamba Solver** for faster dependency resolution" >> performance-report.md
            echo "5. **Use Smaller Channels** when possible for faster resolution" >> performance-report.md
            echo "" >> performance-report.md
            echo "### ğŸ”§ Configuration Recommendations" >> performance-report.md
            echo "" >> performance-report.md
            echo "**For Speed-Focused CI/CD:**" >> performance-report.md
            echo "- Native conda + package caching + mamba solver" >> performance-report.md
            echo "- Custom channel with only required packages" >> performance-report.md
            echo "- Environment updates instead of fresh creation" >> performance-report.md
            echo "" >> performance-report.md
            echo "**For Reproducible Deployments:**" >> performance-report.md
            echo "- conda-lock lockfiles for exact reproducibility" >> performance-report.md
            echo "- Environment caching to avoid repeated resolution" >> performance-report.md
            echo "- Integrated setup for simplicity" >> performance-report.md
            echo "" >> performance-report.md
            echo "**For Development Workflows:**" >> performance-report.md
            echo "- setup-miniconda action for cross-platform consistency" >> performance-report.md
            echo "- Standard environment.yml for flexibility" >> performance-report.md
            echo "- Package caching for faster iteration" >> performance-report.md
            echo "" >> performance-report.md
          else
            echo "### âš ï¸ Insufficient Data for Recommendations" >> performance-report.md
            echo "" >> performance-report.md
            echo "**Run more tests to get comprehensive recommendations:**" >> performance-report.md
            echo "" >> performance-report.md
            echo '```bash' >> performance-report.md
            echo '# Run all performance tests' >> performance-report.md
            echo 'gh workflow run main-performance-test.yml -f test-suite=all' >> performance-report.md
            echo '' >> performance-report.md
            echo '# Then regenerate summary' >> performance-report.md
            echo 'gh workflow run performance-summary.yml' >> performance-report.md
            echo '```' >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Next Steps
          echo "## ğŸ“‹ Next Steps" >> performance-report.md
          echo "" >> performance-report.md
          
          if [ $SUCCESS_COUNT -lt 7 ]; then
            echo "### Missing Test Data" >> performance-report.md
            echo "" >> performance-report.md
            [ "$HAS_CORE" != "success" ] && echo "- âŒ **Core Setup Comparison**: \`gh workflow run core-setup-comparison.yml\`" >> performance-report.md
            [ "$HAS_ENV" != "success" ] && echo "- âŒ **Environment Operations**: \`gh workflow run environment-operations.yml\`" >> performance-report.md
            [ "$HAS_CACHE" != "success" ] && echo "- âŒ **Caching Performance**: \`gh workflow run caching-performance.yml\`" >> performance-report.md
            [ "$HAS_SHELL" != "success" ] && echo "- âŒ **Shell Performance**: \`gh workflow run shell-performance.yml\`" >> performance-report.md
            [ "$HAS_SOLVER" != "success" ] && echo "- âŒ **Solver Optimization**: \`gh workflow run solver-optimization.yml\`" >> performance-report.md
            [ "$HAS_LOCKFILE" != "success" ] && echo "- âŒ **Lockfile Performance**: \`gh workflow run lockfile-performance.yml\`" >> performance-report.md
            [ "$HAS_CHANNEL" != "success" ] && echo "- âŒ **Channel Performance**: \`gh workflow run channel-performance.yml\`" >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          echo "### Implementation Checklist" >> performance-report.md
          echo "" >> performance-report.md
          echo "1. **âœ… Analyze Results**: Review performance data above" >> performance-report.md
          echo "2. **ğŸ”§ Apply Optimizations**: Implement recommended changes" >> performance-report.md
          echo "3. **ğŸ“Š Measure Impact**: Re-run tests after optimization" >> performance-report.md
          echo "4. **ğŸ”„ Iterate**: Continue optimizing based on results" >> performance-report.md
          echo "" >> performance-report.md
          echo "### Performance Monitoring" >> performance-report.md
          echo "" >> performance-report.md
          echo "- **ğŸ”„ Regular Testing**: Run monthly to track performance trends" >> performance-report.md
          echo "- **ğŸ“ˆ Track Metrics**: Monitor setup times, cache hit rates, resolution speed" >> performance-report.md
          echo "- **ğŸ¯ Set Targets**: Define acceptable performance thresholds" >> performance-report.md
          echo "- **âš¡ Optimize Continuously**: Apply new optimizations as they become available" >> performance-report.md
          echo "" >> performance-report.md
          echo "---" >> performance-report.md
          echo "" >> performance-report.md
          echo "**Report Details:**" >> performance-report.md
          echo "- ğŸ“Š Generated from $SUCCESS_COUNT/7 successful test runs" >> performance-report.md
          echo "- ğŸ”— Links to detailed results provided above" >> performance-report.md
          echo "- ğŸ¯ Recommendations based on actual performance measurements" >> performance-report.md
          echo "- ğŸ“‹ Action items prioritized by impact" >> performance-report.md

      - name: Upload Performance Report
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis-report-${{ github.run_number }}
          path: performance-report.md
          retention-days: 90

      - name: Display Summary
        run: |
          echo "=== PERFORMANCE ANALYSIS COMPLETE ==="
          echo ""
          echo "ğŸ“Š Report generated with analysis of available test data"
          echo "ğŸ“ Download detailed report from artifacts"
          echo "ğŸ”— Individual test results linked in report"
          echo ""
          echo "Quick Summary:"
          cat performance-report.md | head -30
          echo ""
          echo "... (see full report in artifacts)"
