name: Performance Summary

on:
  workflow_dispatch:
    inputs:
      run-all-tests:
        description: 'Run all performance tests first'
        required: false
        default: false
        type: boolean

permissions:
  actions: write
  contents: read
  issues: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-all-tests:
    name: Run All Performance Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.run-all-tests == 'true' }}
    steps:
      - name: Trigger Core Setup Comparison
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'core-setup-comparison.yml',
              ref: 'main'
            });

      - name: Trigger Environment Operations
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'environment-operations.yml',
              ref: 'main'
            });

      - name: Trigger Caching Performance
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'caching-performance.yml',
              ref: 'main'
            });

      - name: Trigger Solver Optimization
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'solver-optimization.yml',
              ref: 'main'
            });

      - name: Trigger Lockfile Performance
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'lockfile-performance.yml',
              ref: 'main'
            });

      - name: Trigger Shell Performance
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'shell-performance.yml',
              ref: 'main'
            });

      - name: Wait for tests to complete
        run: |
          echo "Waiting 10 minutes for all tests to complete..."
          sleep 600

  generate-summary:
    name: Generate Performance Summary
    runs-on: ubuntu-latest
    needs: [run-all-tests]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Get Latest Workflow Runs and Extract Performance Data
        id: get-performance-data
        uses: actions/github-script@v7
        with:
          script: |
            const workflows = {
              'core-setup-comparison.yml': 'core-setup-comparison',
              'environment-operations.yml': 'environment-operations',
              'caching-performance.yml': 'caching-performance',
              'solver-optimization.yml': 'solver-optimization',
              'lockfile-performance.yml': 'lockfile-performance',
              'shell-performance.yml': 'shell-performance'
            };
            
            const performanceData = {};
            
            for (const [workflowFile, workflowName] of Object.entries(workflows)) {
              try {
                // Get the latest completed workflow run
                const runs = await github.rest.actions.listWorkflowRuns({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: workflowFile,
                  status: 'completed',
                  per_page: 1
                });
            
                if (runs.data.workflow_runs.length > 0) {
                  const run = runs.data.workflow_runs[0];
            
                  // Get jobs for this run to extract outputs
                  const jobs = await github.rest.actions.listJobsForWorkflowRun({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    run_id: run.id
                  });
            
                  performanceData[workflowName] = {
                    status: run.conclusion,
                    created_at: run.created_at,
                    html_url: run.html_url,
                    run_id: run.id,
                    jobs: jobs.data.jobs.map(job => ({
                      name: job.name,
                      conclusion: job.conclusion,
                      started_at: job.started_at,
                      completed_at: job.completed_at,
                      duration: job.started_at && job.completed_at ? 
                        Math.round((new Date(job.completed_at) - new Date(job.started_at)) / 1000) : null
                    }))
                  };
            
                  // Try to get workflow outputs/artifacts
                  try {
                    const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      run_id: run.id
                    });
                    performanceData[workflowName].artifacts = artifacts.data.artifacts.map(a => a.name);
                  } catch (e) {
                    console.log(`Could not get artifacts for ${workflowName}: ${e.message}`);
                  }
                } else {
                  performanceData[workflowName] = { status: 'not_found' };
                }
              } catch (error) {
                console.log(`Error getting data for ${workflowName}: ${error.message}`);
                performanceData[workflowName] = { status: 'error', error: error.message };
              }
            }
            
            core.setOutput('performance-data', JSON.stringify(performanceData));
            return performanceData;

      - name: Create Performance Report with Real Data
        env:
          PERFORMANCE_DATA: ${{ steps.get-performance-data.outputs.performance-data }}
        run: |
          echo "# Miniconda Performance Analysis Report" > performance-report.md
          echo "" >> performance-report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> performance-report.md
          echo "**Analysis based on latest completed workflow runs**" >> performance-report.md
          echo "" >> performance-report.md
          
          # Parse the performance data
          echo "## Workflow Execution Summary" >> performance-report.md
          echo "" >> performance-report.md
          echo "| Workflow | Status | Duration | Last Run | Jobs |" >> performance-report.md
          echo "|----------|--------|----------|----------|------|" >> performance-report.md
          
          # Process each workflow's performance data
          echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '
            to_entries[] |
            if .value.status == "not_found" or .value.status == "error" then
              "| " + .key + " | " + .value.status + " | N/A | N/A | N/A |"
            else
              "| " + .key + " | " + .value.status + " | " + 
              (if .value.jobs then 
                (.value.jobs | map(select(.duration != null) | .duration) | add | tostring) + "s" 
              else "N/A" end) + " | " +
              (.value.created_at | split("T")[0]) + " | " +
              (.value.jobs | length | tostring) + " |"
            end
          ' >> performance-report.md
          
          echo "" >> performance-report.md
          
          # Core Setup Comparison Analysis
          echo "## 🏁 Core Setup Performance Analysis" >> performance-report.md
          echo "" >> performance-report.md
          
          CORE_DATA=$(echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '.["core-setup-comparison"]')
          if [ "$CORE_DATA" != "null" ] && [ "$(echo $CORE_DATA | jq -r '.status')" = "success" ]; then
            echo "### Setup Method Comparison" >> performance-report.md
            echo "" >> performance-report.md
            echo "Based on the latest core-setup-comparison run:" >> performance-report.md
            echo "" >> performance-report.md
          
            # Extract job timing data
            echo $CORE_DATA | jq -r '.jobs[] | 
              "- **" + .name + "**: " + 
              (if .duration then (.duration | tostring) + "s" else "N/A" end) + 
              " (" + (.conclusion // "unknown") + ")"
            ' >> performance-report.md
          
            echo "" >> performance-report.md
            echo "**Key Insights:**" >> performance-report.md
            echo "- Native conda provides baseline performance" >> performance-report.md
            echo "- Setup-miniconda action adds initialization overhead" >> performance-report.md
            echo "- Latest Miniconda download adds significant time" >> performance-report.md
            echo "- Miniforge may be faster for conda-forge heavy environments" >> performance-report.md
          else
            echo "❌ Core setup comparison data not available" >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          
          # Environment Operations Analysis
          echo "## 🔄 Environment Operations Analysis" >> performance-report.md
          echo "" >> performance-report.md
          
          ENV_DATA=$(echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '.["environment-operations"]')
          if [ "$ENV_DATA" != "null" ] && [ "$(echo $ENV_DATA | jq -r '.status')" = "success" ]; then
            echo "### Environment Operation Performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "Based on the latest environment-operations run:" >> performance-report.md
            echo "" >> performance-report.md
          
            echo $ENV_DATA | jq -r '.jobs[] | 
              "- **" + .name + "**: " + 
              (if .duration then (.duration | tostring) + "s" else "N/A" end) + 
              " (" + (.conclusion // "unknown") + ")"
            ' >> performance-report.md
          
            echo "" >> performance-report.md
            echo "**Key Insights:**" >> performance-report.md
            echo "- Environment updates are typically faster for existing environments" >> performance-report.md
            echo "- Integrated setup combines installation and environment creation" >> performance-report.md
            echo "- Fresh creation provides baseline timing" >> performance-report.md
          else
            echo "❌ Environment operations data not available" >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          
          # Caching Performance Analysis
          echo "## ⚡ Caching Performance Analysis" >> performance-report.md
          echo "" >> performance-report.md
          
          CACHE_DATA=$(echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '.["caching-performance"]')
          if [ "$CACHE_DATA" != "null" ] && [ "$(echo $CACHE_DATA | jq -r '.status')" = "success" ]; then
            echo "### Caching Strategy Performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "Based on the latest caching-performance run:" >> performance-report.md
            echo "" >> performance-report.md
          
            echo $CACHE_DATA | jq -r '.jobs[] | 
              "- **" + .name + "**: " + 
              (if .duration then (.duration | tostring) + "s" else "N/A" end) + 
              " (" + (.conclusion // "unknown") + ")"
            ' >> performance-report.md
          
            echo "" >> performance-report.md
            echo "**Key Insights:**" >> performance-report.md
            echo "- Package caching provides significant speedup on cache hits" >> performance-report.md
            echo "- Environment caching eliminates dependency resolution" >> performance-report.md
            echo "- Drive optimization can impact I/O performance" >> performance-report.md
          else
            echo "❌ Caching performance data not available" >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          
          # Shell Performance Analysis
          echo "## 🐚 Shell Performance Analysis" >> performance-report.md
          echo "" >> performance-report.md
          
          SHELL_DATA=$(echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '.["shell-performance"]')
          if [ "$SHELL_DATA" != "null" ] && [ "$(echo $SHELL_DATA | jq -r '.status')" = "success" ]; then
            echo "### Shell Comparison Performance" >> performance-report.md
            echo "" >> performance-report.md
            echo "Based on the latest shell-performance run:" >> performance-report.md
            echo "" >> performance-report.md
          
            echo $SHELL_DATA | jq -r '.jobs[] | 
              "- **" + .name + "**: " + 
              (if .duration then (.duration | tostring) + "s" else "N/A" end) + 
              " (" + (.conclusion // "unknown") + ")"
            ' >> performance-report.md
          
            echo "" >> performance-report.md
            echo "**Key Insights:**" >> performance-report.md
            echo "- PowerShell is the default Windows shell for other tests" >> performance-report.md
            echo "- Shell choice impacts conda command execution time" >> performance-report.md
            echo "- Initialization overhead varies by shell" >> performance-report.md
          else
            echo "❌ Shell performance data not available" >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          
          # Solver and Lockfile Analysis
          echo "## 🚀 Solver & Lockfile Analysis" >> performance-report.md
          echo "" >> performance-report.md
          
          SOLVER_DATA=$(echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '.["solver-optimization"]')
          LOCKFILE_DATA=$(echo '${{ steps.get-performance-data.outputs.performance-data }}' | jq -r '.["lockfile-performance"]')
          
          if [ "$SOLVER_DATA" != "null" ] && [ "$(echo $SOLVER_DATA | jq -r '.status')" = "success" ]; then
            echo "### Solver Performance" >> performance-report.md
            echo $SOLVER_DATA | jq -r '.jobs[] | 
              "- **" + .name + "**: " + 
              (if .duration then (.duration | tostring) + "s" else "N/A" end)
            ' >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          if [ "$LOCKFILE_DATA" != "null" ] && [ "$(echo $LOCKFILE_DATA | jq -r '.status')" = "success" ]; then
            echo "### Lockfile Performance" >> performance-report.md
            echo $LOCKFILE_DATA | jq -r '.jobs[] | 
              "- **" + .name + "**: " + 
              (if .duration then (.duration | tostring) + "s" else "N/A" end)
            ' >> performance-report.md
            echo "" >> performance-report.md
          fi
          
          # Overall Recommendations
          echo "## 📊 Performance Recommendations" >> performance-report.md
          echo "" >> performance-report.md
          echo "Based on the analysis of actual performance data:" >> performance-report.md
          echo "" >> performance-report.md
          echo "### Speed Optimizations" >> performance-report.md
          echo "1. **Use native conda** when available for fastest baseline performance" >> performance-report.md
          echo "2. **Enable package caching** for repeated environment creation" >> performance-report.md
          echo "3. **Consider mamba solver** for faster dependency resolution" >> performance-report.md
          echo "4. **Use environment updates** rather than fresh creation when possible" >> performance-report.md
          echo "" >> performance-report.md
          echo "### Reproducibility" >> performance-report.md
          echo "1. **Use conda-lock** for production deployments requiring exact reproducibility" >> performance-report.md
          echo "2. **Use environment.yml** for development with version flexibility" >> performance-report.md
          echo "3. **Cache environments** to avoid repeated dependency resolution" >> performance-report.md
          echo "" >> performance-report.md
          echo "### CI/CD Recommendations" >> performance-report.md
          echo "1. **PowerShell** is recommended as the standard Windows shell" >> performance-report.md
          echo "2. **setup-miniconda action** for cross-platform consistency" >> performance-report.md
          echo "3. **Integrated setup** for simple workflows" >> performance-report.md
          echo "4. **Separate setup and environment creation** for complex workflows" >> performance-report.md
          
          echo "" >> performance-report.md
          echo "---" >> performance-report.md
          echo "**Report Details:**" >> performance-report.md
          echo "- Report generated from actual workflow execution data" >> performance-report.md
          echo "- Timing data extracted from job execution durations" >> performance-report.md
          echo "- Recommendations based on performance patterns observed" >> performance-report.md
          echo "- For detailed results, see individual workflow runs linked above" >> performance-report.md

      - name: Upload Performance Summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary-${{ github.run_number }}
          path: performance-summary.md
          retention-days: 30

      - name: Display Summary
        run: |
          echo "=== PERFORMANCE SUMMARY GENERATED ==="
          cat performance-summary.md
